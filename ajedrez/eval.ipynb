{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f46e227",
   "metadata": {},
   "source": [
    "En este archivo se explica e implementa la evaluación que los diferentes modelos dan a los conjuntos de test, y se calcula el rendimiento que estos modelos obtienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875b7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b5b97",
   "metadata": {},
   "source": [
    "Definimos los métodos, versiones del *conjunto de datos* y las *métricas* que vamos a usar, así como los *nombres de las clases objetivo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bfa35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contiene los nombres de las clases\n",
    "TARGET_NAMES = [\"1-0\", \"0-1\", \"1/2-1/2\"]\n",
    "\n",
    "# Nombre de todos los métodos \n",
    "METHODS = ['knn', 'svm', 'naive_bayes', 'random_forest']\n",
    "\n",
    "# Nombre de todos los conjuntos de datos\n",
    "SETS = [\n",
    "    'normalized', 'normalized_PCA80', 'normalized_PCA95',\n",
    "    'original', 'original_PCA80', 'original_PCA95',\n",
    "    'standard', 'standard_PCA80', 'standard_PCA95'\n",
    "]\n",
    "\n",
    "# Nombre de todas las métricas a calcular\n",
    "METRICS_NAMES = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1_Score', 'FNR', 'FPR', 'AUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece95ba6",
   "metadata": {},
   "source": [
    "Creación de directorios necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4218974",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = os.getcwd()\n",
    "PREDICTIONS_PATH = os.path.join(CWD, \"predictions\")\n",
    "IMAGES_PATH = os.path.join(CWD, \"roc-curve-images\")\n",
    "DATA_PATH = os.path.join(CWD, \"data\")\n",
    "METRICS_PATH = os.path.join(CWD, \"metrics\")\n",
    "\n",
    "MODELS_PATH = os.path.join(CWD, \"models\") # Este se creó en train.ipynb\n",
    "\n",
    "try:\n",
    "    os.mkdir(PREDICTIONS_PATH)\n",
    "    os.mkdir(IMAGES_PATH)\n",
    "    os.mkdir(DATA_PATH)\n",
    "    os.mkdir(METRICS_PATH)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5499594",
   "metadata": {},
   "source": [
    "### Rendimiento del modelo\n",
    "Para llevar a cabo la evaluación, un modelo entrenado debe asignar una clase a cada instancia del conjunto de test. Se implementa a continuación una función que, para cada versión ($set$) y partición ($fold$) del conjunto de datos, calcula las métricas de rendimiento de un modelo ($model$) entrenado con el método seleccionado (method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ca2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(\n",
    "    model: KNeighborsClassifier | SVC | GaussianNB | RandomForestClassifier,\n",
    "    set: str, \n",
    "    fold: int,\n",
    "    method_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Esta función se encarga de:\n",
    "    - Hacer las predicciones de un modelo sobre un conjunto de test y fold determinado\n",
    "    - Guardar las probabilidades de pertenencia a cada clase en formato csv\n",
    "    - Generar curvas ROC y el área bajo la curva\n",
    "    - Generar métricas accuracy, precision, etc. y las devuelve como una lista\n",
    "    Nótese que las métricas son calculadas \"One-vs-rest\", es decir, \n",
    "    se calculan todas las métricas como si fuera un problema binario\n",
    "    considerando una de las clases como positiva y las otras dos negativas\n",
    "    y luego se computa la media de las métricas obtenidas\n",
    "    \"\"\"\n",
    "    print(f\"model: {method_name}, set: {set}, fold: {fold}\")\n",
    "    \n",
    "    # Cargamos el conjunto de datos de test del fold correspondiente\n",
    "    full_path = os.path.join(DATA_PATH, set, f\"test{fold}_{set}.csv\")\n",
    "    test_df = pd.read_csv(full_path)\n",
    "\n",
    "    X_test = test_df.iloc[:, :-1]  # Atributos de entrada\n",
    "    y_test = test_df.iloc[:, -1]  \n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Probabilidades de pertenencia a cada clase\n",
    "    y_scores = pd.DataFrame(model.predict_proba(X_test))\n",
    "    y_scores.to_csv(os.path.join(PREDICTIONS_PATH, f\"pred_{fold}_{set}_{method_name}.csv\"), index=False)\n",
    "    \n",
    "    # Curvas ROC del random forest:\n",
    "    if method_name == \"random_forest\":\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "        for class_id in range(3):\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_scores.iloc[ : , class_id], pos_label=class_id)  \n",
    "            ax.plot(fpr, tpr, label=f\"ROC curve - Positive class: {TARGET_NAMES[class_id]}\")\n",
    "\n",
    "        ax.set(\n",
    "            xlabel=\"False Positive Rate\",\n",
    "            ylabel=\"True Positive Rate\",\n",
    "            title=\"Curva ROC One-vs-Rest\"\n",
    "        )\n",
    "        \n",
    "        ax.legend()\n",
    "        fig.savefig(os.path.join(IMAGES_PATH, f\"ROC_{set}_{fold}.png\"))\n",
    "        plt.close(fig) # No mostrar la figura\n",
    "\n",
    "    # Área debajo de la curva ROC\n",
    "    roc_auc = roc_auc_score(y_test, y_scores, multi_class='ovr')\n",
    "    \n",
    "    # Resto de métricas\n",
    "    cm = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    tn = cm[ : , 0, 0]\n",
    "    tp = cm[ : , 1, 1]\n",
    "    fn = cm[ : , 1, 0]\n",
    "    fp = cm[ : , 0, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    sensitivity = np.mean(tp / (tp + fn))\n",
    "    recall = sensitivity\n",
    "    precision = np.mean(tp / (tp + fp))\n",
    "    fnr = np.mean(fn / (fn + tp))\n",
    "    fpr = np.mean(fp / (fp + tn))\n",
    "    specificity = 1 - fpr\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return [accuracy, sensitivity, specificity, precision, f1, fnr, fpr, roc_auc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33767c7f",
   "metadata": {},
   "source": [
    "### Generación de métricas de modelos\n",
    "A continuación, para cada versión y partición de los datos se va a generar las métricas de cada uno de los modelos generados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "394c445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: knn, set: normalized, fold: 1\n",
      "     0    1    2    3\n",
      "0  0.0  0.2  0.8  0.0\n",
      "1  0.0  0.6  0.4  0.0\n",
      "2  0.0  0.2  0.8  0.0\n",
      "3  0.0  0.4  0.6  0.0\n",
      "4  0.0  0.2  0.8  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Primer_Cuatri\\Aprendizaje_Automatico\\practicas-aprendizaje\\venv\\Lib\\site-packages\\sklearn\\base.py:463: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.7.2 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes in y_true not equal to the number of columns in 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     model = joblib.load(model_path)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Usamos la función definida anteriormente para obtener las métricas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     metrics = \u001b[43mmake_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     fold_metrics.append(metrics)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Creamos el DataFrame con los resultados de los folds\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mmake_predictions\u001b[39m\u001b[34m(model, set, fold, method_name)\u001b[39m\n\u001b[32m     51\u001b[39m     plt.close(fig) \u001b[38;5;66;03m# No mostrar la figura\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Área debajo de la curva ROC\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m roc_auc = \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43movr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Resto de métricas\u001b[39;00m\n\u001b[32m     57\u001b[39m cm = multilabel_confusion_matrix(y_test, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Primer_Cuatri\\Aprendizaje_Automatico\\practicas-aprendizaje\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Primer_Cuatri\\Aprendizaje_Automatico\\practicas-aprendizaje\\venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:698\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multi_class == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    697\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmulti_class must be in (\u001b[39m\u001b[33m'\u001b[39m\u001b[33movo\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33movr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    702\u001b[39m     labels = np.unique(y_true)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Primer_Cuatri\\Aprendizaje_Automatico\\practicas-aprendizaje\\venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:815\u001b[39m, in \u001b[36m_multiclass_roc_auc_score\u001b[39m\u001b[34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[39m\n\u001b[32m    813\u001b[39m     classes = _unique(y_true)\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes) != y_score.shape[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    816\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes in y_true not equal to the number of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    817\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcolumns in \u001b[39m\u001b[33m'\u001b[39m\u001b[33my_score\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    818\u001b[39m         )\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multi_class == \u001b[33m\"\u001b[39m\u001b[33movo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Number of classes in y_true not equal to the number of columns in 'y_score'"
     ]
    }
   ],
   "source": [
    "for method in METHODS:\n",
    "    for set in SETS:\n",
    "        \n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold in range(1, 6):\n",
    "            \n",
    "            # Cargamos el modelo correspondiente\n",
    "            model_name = f\"{method}_{set}_{fold}.pkl\"\n",
    "            model_path = os.path.join(MODELS_PATH, model_name)\n",
    "            model = joblib.load(model_path)\n",
    "\n",
    "            # Usamos la función definida anteriormente para obtener las métricas\n",
    "            metrics = make_predictions(model, set, fold, method)\n",
    "\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "        # Creamos el DataFrame con los resultados de los folds\n",
    "        df_metrics = pd.DataFrame(fold_metrics, columns=METRICS_NAMES)\n",
    "\n",
    "        # Exportamos a csv\n",
    "        full_path = os.path.join(METRICS_PATH, f\"{method}_{set}_metrics.csv\")\n",
    "        df_metrics.to_csv(full_path, index=True, index_label=\"fold\")\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
