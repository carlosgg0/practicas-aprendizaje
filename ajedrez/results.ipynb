{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae8e874",
   "metadata": {},
   "source": [
    "En este archivo se mostraran los resúmenes de las métricas de rendimiento que cada modelo ha obtenido en los procesos anteriores. Además, se implementarán distintos \"ensemble\" de los modelos, para obtener resultados globales con todos los modelos entrenados. \n",
    "\n",
    "Primero de todo importamos las librerías que vamos a utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd86001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a79df4",
   "metadata": {},
   "source": [
    "Declaramos constantes globales, como pueden ser los *métodos*, *conjuntos de datos*, *nombres de las métricas* así como *índices de las clases*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e704b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = ['knn', 'svm', 'naive_bayes', 'random_forest']\n",
    "SETS = [\n",
    "    'normalized', 'normalized_pca_80', 'normalized_pca_95',\n",
    "    'original', 'original_pca_80', 'original_pca_95',\n",
    "    'standarized', 'standarized_pca_80', 'standarized_pca_95'\n",
    "]\n",
    "METRICS_NAMES = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1_Score', 'FNR', 'FPR', 'AUC']\n",
    "TARGET_NAMES = ['0-1', '1-0', '1/2-1/2']\n",
    "CLASSES = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d429533",
   "metadata": {},
   "source": [
    "Creamos los directorios necesarios para almacenar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "097c027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de directorios necesarios\n",
    "CWD = os.getcwd()\n",
    "RESULTS_PATH = os.path.join(CWD, \"results\")\n",
    "FIGURES_PATH = os.path.join(CWD, \"figures\")\n",
    "DATA_PATH = os.path.join(CWD, \"data\")\n",
    "METRICS_PATH = os.path.join(CWD, \"metrics\")\n",
    "\n",
    "try:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "    os.mkdir(FIGURES_PATH)\n",
    "except FileExistsError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385af1d",
   "metadata": {},
   "source": [
    "## Función\n",
    "Creamos una función `calcula_media_desv_tipica` para ahorrarnos repetir código en la generación de los *.csv* con los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba2a7e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\gonza\\AppData\\Local\\Temp\\ipykernel_23384\\767665287.py:13: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  results.append(f\"{mean[i]:.3f} $\\pm$ {std[i]:.3f}\")\n"
     ]
    }
   ],
   "source": [
    "def calcula_media_desv_tipica(method: str, set: str):\n",
    "\n",
    "    # Cargamos las métricas del modelo y conjunto correspondientes\n",
    "    full_path = os.path.join(METRICS_PATH, f\"{method}_{set}_metrics.csv\")\n",
    "    metrics = pd.read_csv(full_path, index_col=\"fold\")\n",
    "    \n",
    "    # Calculamos media y desv. típica de cada columna\n",
    "    mean = metrics.mean().to_numpy()\n",
    "    std = metrics.std().to_numpy()\n",
    "\n",
    "    results = [f\"{set}\"]\n",
    "    for i in range(0, mean.shape[0]):\n",
    "        results.append(f\"{mean[i]:.3f} $\\pm$ {std[i]:.3f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a04b89",
   "metadata": {},
   "source": [
    "## Generación de los resultados\n",
    "Utilizando la función anteriormente mencionada, así como iterando entre los distintos modelos, obtenemos los distintos resultados que se piden en el *enunciado*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e64900",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_summary_path_tex_ori = os.path.join(RESULTS_PATH, f\"ori_f1_table.tex\")\n",
    "f1_summary_path_tex_norm = os.path.join(RESULTS_PATH, f\"norm_f1_table.tex\")\n",
    "f1_summary_path_tex_stand = os.path.join(RESULTS_PATH, f\"stand_f1_table.tex\")\n",
    "\n",
    "f1_summary_path_csv = os.path.join(RESULTS_PATH, f\"f1_table.csv\")\n",
    "\n",
    "# Tabla para almacenar los f1-score de cada modelo en cada conjunto de datos\n",
    "f1_summary_table = []\n",
    "\n",
    "for method in METHODS:\n",
    "    \n",
    "    # Generamos la tabla con las métricas del método correspondiente\n",
    "    tabla_metodo = []\n",
    "    for set in SETS:\n",
    "        fila = calcula_media_desv_tipica(method, set)\n",
    "        tabla_metodo.append(fila)\n",
    "\n",
    "    # Generamos un dataframe a partir de dicha tabla\n",
    "    df = pd.DataFrame(tabla_metodo, columns=[\"Set\", *METRICS_NAMES])\n",
    "    \n",
    "    # Guardamos los resultados de la tabla en formato latex y csv\n",
    "    csv_path = os.path.join(RESULTS_PATH, f\"{method}.csv\")\n",
    "    tex_path = os.path.join(RESULTS_PATH, f\"{method}.tex\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_latex(tex_path, index=False, caption=f\"Resultados para {method}\")\n",
    "\n",
    "    f1_values = df[\"F1_Score\"].to_list()\n",
    "    f1_summary_table.append(f1_values)\n",
    "\n",
    "f1_summary_table_df = pd.DataFrame(f1_summary_table, columns=SETS, index=METHODS)\n",
    "\n",
    "# Guardamos la tabla con los f1-scores en formatos latex y csv\n",
    "f1_summary_table_df.to_csv(f1_summary_path_csv)\n",
    "\n",
    "f1_summary_table_df[['normalized', 'normalized_pca_80', 'normalized_pca_95']] \\\n",
    "    .to_latex(f1_summary_path_tex_norm, caption=\"Resumen F1-Score por Método y Datasets normalizados\")\n",
    "\n",
    "f1_summary_table_df[['original', 'original_pca_80', 'original_pca_95']] \\\n",
    "    .to_latex(f1_summary_path_tex_ori, caption=\"Resumen F1-Score por Método y Dataset originales\")\n",
    "\n",
    "f1_summary_table_df[['standarized', 'standarized_pca_80', 'standarized_pca_95']] \\\n",
    "    .to_latex(f1_summary_path_tex_stand, caption=\"Resumen F1-Score por Método y Dataset estandarizados\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fdd6ef",
   "metadata": {},
   "source": [
    "En el siguiente bloque de código obtenemos las gráficas pedidas de, por ejemplo, *FN contra FP*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "244ef6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = []\n",
    "\n",
    "for method in METHODS:\n",
    "    for dataset in SETS:\n",
    "        full_path = os.path.join(METRICS_PATH, f\"{method}_{dataset}_metrics.csv\")\n",
    "        \n",
    "        if os.path.exists(full_path):\n",
    "            metrics = pd.read_csv(full_path, index_col=\"fold\")\n",
    "            means = metrics.mean()\n",
    "            \n",
    "            entry = {\n",
    "                'Method': method,\n",
    "                'Set': dataset,\n",
    "                'FP': means.get('FPR', 0), \n",
    "                'FN': means.get('FNR', 0), \n",
    "                'PR': means.get('Precision', 0),\n",
    "                'RC': means.get('Sensitivity', 0),\n",
    "                'ACC': means.get('Accuracy', 0),\n",
    "                'Fm': means.get('F1_Score', 0)\n",
    "            }\n",
    "            plot_data.append(entry)\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "# 2. Función para dibujar las líneas\n",
    "def plot_lines_comparison(method_name, df_method, metric1, metric2, label1, label2, title, filename):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Eje X: Los datasets en el orden definido en SETS\n",
    "    x_values = df_method['Set']\n",
    "    y1_values = df_method[metric1]\n",
    "    y2_values = df_method[metric2]\n",
    "    \n",
    "    # Dibujamos las dos líneas\n",
    "    plt.plot(x_values, y1_values, marker='o', linestyle='-', linewidth=2, label=label1, color='blue')\n",
    "    plt.plot(x_values, y2_values, marker='s', linestyle='--', linewidth=2, label=label2, color='orange')\n",
    "    \n",
    "    plt.title(f\"{title} - {method_name}\", fontsize=14)\n",
    "    plt.xlabel(\"Conjunto de Datos\", fontsize=12)\n",
    "    plt.ylabel(\"Valor Métrica\", fontsize=12)\n",
    "    plt.ylim(-0.05, 1.05) # Fijamos el eje Y entre 0 y 1 para ver mejor las diferencias\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotamos las etiquetas del eje X para que se lean bien\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    save_path = os.path.join(FIGURES_PATH, filename)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# 3. Bucle principal\n",
    "for method in METHODS:\n",
    "    # Filtramos los datos solo para el método actual\n",
    "    # Reindexamos por SETS para asegurar que el eje X siga el orden lógico (Norm -> Norm_PCA...)\n",
    "    # y no el orden alfabético aleatorio.\n",
    "    df_method = df_plot[df_plot['Method'] == method].set_index('Set').reindex(SETS).reset_index()\n",
    "    \n",
    "    # a) ACC contra Fm\n",
    "    plot_lines_comparison(\n",
    "        method, df_method, \n",
    "        'ACC', 'Fm', \n",
    "        'Accuracy', 'F1-Score', \n",
    "        'Comparativa Accuracy vs F1-Score', \n",
    "        f\"ACC_vs_Fm_{method}.png\"\n",
    "    )\n",
    "    \n",
    "    # b) FN contra FP\n",
    "    plot_lines_comparison(\n",
    "        method, df_method, \n",
    "        'FN', 'FP', \n",
    "        'False Negative Rate', 'False Positive Rate', \n",
    "        'Comparativa FN vs FP', \n",
    "        f\"FN_vs_FP_{method}.png\"\n",
    "    )\n",
    "    \n",
    "    # c) PR contra RC\n",
    "    plot_lines_comparison(\n",
    "        method, df_method, \n",
    "        'PR', 'RC', \n",
    "        'Precision', 'Recall', \n",
    "        'Comparativa Precision vs Recall', \n",
    "        f\"PR_vs_RC_{method}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b64f55",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "En este apartado generaremos los distintos **ensembles** que se piden: decisión por votación, media y mediana. Antes de todo, creamos una función que leerá los *csv* con las predicciones para ahorrar repetición de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1bd8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_probabilidad(model_name, fold, set):\n",
    "    \"\"\"\n",
    "    Obtienes el dataframe de probabilidades del modelo, fold y set especificado\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo (e.g., 'knn').\n",
    "        fold (int): Número de la partición (e.g., 1).\n",
    "        set_name (str): Versión del conjunto de datos (e.g., 'norm').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con los resultados, o None si el archivo no existe.\n",
    "    \"\"\"\n",
    "    # Obtenemos el path del csv y lo cargamos\n",
    "    file_name = f\"pred_{fold}_{set}_{model_name}.csv\"\n",
    "    full_path = os.path.join(\"predictions\", file_name)\n",
    "\n",
    "    predictions_df = pd.read_csv(full_path)\n",
    "\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ea581ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generando Ensembles: VOTES ---\n",
      "Completado votes.\n",
      "\n",
      "--- Generando Ensembles: AVG ---\n",
      "Completado avg.\n",
      "\n",
      "--- Generando Ensembles: MEDIAN ---\n",
      "Completado median.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generar_ensemble(strategy: str, sets: list, methods: list, classes: list):\n",
    "    \"\"\"\n",
    "    Genera los archivos de ensemble según la estrategia dada.\n",
    "    \n",
    "    Args:\n",
    "        strategy (str): 'votes', 'avg', o 'median'.\n",
    "        sets (list): Lista de conjuntos de datos.\n",
    "        methods (list): Lista de modelos.\n",
    "        classes (list): Lista de nombres de clases.\n",
    "    \"\"\"\n",
    "    folder_map = {\n",
    "        'votes': 'ensemble_votes',\n",
    "        'avg': 'ensemble_avg',\n",
    "        'median': 'ensemble_median'\n",
    "    }\n",
    "    folder_name = folder_map.get(strategy)\n",
    "    \n",
    "    # Crear directorio si no existe\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    print(f\"--- Generando Ensembles: {strategy.upper()} ---\")\n",
    "\n",
    "    for dataset in sets:\n",
    "        for fold in range(1, 6):\n",
    "            prediction_dfs = []\n",
    "\n",
    "            # Cargar predicciones de todos los modelos\n",
    "            for method in methods:\n",
    "                try:\n",
    "                    df = obtener_probabilidad(method, fold, dataset)\n",
    "                    if df is not None and not df.empty:\n",
    "                        prediction_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error cargando {method} fold {fold}: {e}\")\n",
    "\n",
    "            if not prediction_dfs:\n",
    "                continue\n",
    "\n",
    "            # Agrupamos por el índice original para operar sobre las mismas filas\n",
    "            df_concat = pd.concat(prediction_dfs)\n",
    "            \n",
    "            # DataFrame resultado\n",
    "            df_result = pd.DataFrame(index=prediction_dfs[0].index, columns=classes)\n",
    "            \n",
    "            # Aplicar estrategia\n",
    "            if strategy == 'votes':\n",
    "                # Convertir probabilidades a votos (1 para la clase máxima, 0 resto)\n",
    "                # Para cada modelo, obtenemos la clase predicha\n",
    "                votes_df = pd.DataFrame(index=prediction_dfs[0].index)\n",
    "                for i, df in enumerate(prediction_dfs):\n",
    "                    votes_df[f'model_{i}'] = df[classes].idxmax(axis=1)\n",
    "                \n",
    "                # Contar votos\n",
    "                ensemble_decision = votes_df.mode(axis=1)[0]\n",
    "                \n",
    "                for col in classes:\n",
    "                    df_result[col] = (votes_df == col).sum(axis=1) / len(prediction_dfs)\n",
    "                \n",
    "                df_result['Ensemble_Decision'] = ensemble_decision\n",
    "\n",
    "            elif strategy == 'avg':\n",
    "                # Calcular la media\n",
    "                df_mean = df_concat.groupby(level=0).mean()\n",
    "                df_result[classes] = df_mean[classes].round(3)\n",
    "                df_result['Ensemble_Decision'] = df_result[classes].idxmax(axis=1)\n",
    "\n",
    "            elif strategy == 'median':\n",
    "                # Calcular la mediana\n",
    "                df_median_val = df_concat.groupby(level=0).median()\n",
    "                df_result[classes] = df_median_val[classes].round(3)\n",
    "                df_result['Ensemble_Decision'] = df_result[classes].idxmax(axis=1)\n",
    "\n",
    "            # Guardar resultado\n",
    "            file_name = f\"{strategy}_fold{fold}_{dataset}.csv\"\n",
    "\n",
    "            # Ajuste para nombre de archivo de votación\n",
    "            if strategy == 'votes':\n",
    "                file_name = f\"votes_fold{fold}_{dataset}.csv\"\n",
    "                \n",
    "            full_path = os.path.join(folder_name, file_name)\n",
    "            df_result.to_csv(full_path, index=True, index_label='Index')\n",
    "            \n",
    "    print(f\"Completado {strategy}.\\n\")\n",
    "\n",
    "# Ejecutar para las 3 estrategias\n",
    "generar_ensemble('votes', SETS, METHODS, CLASSES)\n",
    "generar_ensemble('avg', SETS, METHODS, CLASSES)\n",
    "generar_ensemble('median', SETS, METHODS, CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4a2ca",
   "metadata": {},
   "source": [
    "### Métricas ensemble\n",
    "A continuación, vamos a calcular el rendimiento de los ensemble realizados con cada método, para comprobar cual es la mejor técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112fe625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5651317008313957\n",
      "AUC: 0.5642707660672218\n",
      "AUC: 0.5551016749911655\n",
      "AUC: 0.563484386089608\n",
      "AUC: 0.5416826931959021\n",
      "AUC: 0.5111478225098357\n",
      "AUC: 0.5038380640679132\n",
      "AUC: 0.5036665360554153\n",
      "AUC: 0.4926671731293704\n",
      "AUC: 0.5072905070158696\n",
      "AUC: 0.5625692293575809\n",
      "AUC: 0.5488038604200001\n",
      "AUC: 0.5391189452602233\n",
      "AUC: 0.5447858119347245\n",
      "AUC: 0.5396230314411157\n",
      "AUC: 0.5663784190494116\n",
      "AUC: 0.5703559425429945\n",
      "AUC: 0.5525491470890734\n",
      "AUC: 0.5620392161954758\n",
      "AUC: 0.5430957223867002\n",
      "AUC: 0.5090430215586128\n",
      "AUC: 0.47801955473797336\n",
      "AUC: 0.49456375890910875\n",
      "AUC: 0.5069867409662525\n",
      "AUC: 0.48881884837131634\n",
      "AUC: 0.5089975780736696\n",
      "AUC: 0.47801955473797336\n",
      "AUC: 0.49456375890910875\n",
      "AUC: 0.5069867409662525\n",
      "AUC: 0.4884364358858904\n",
      "AUC: 0.5712781094181937\n",
      "AUC: 0.5649769555952339\n",
      "AUC: 0.5566387736264599\n",
      "AUC: 0.5721809088624329\n",
      "AUC: 0.5432235875113246\n",
      "AUC: 0.5514410304638727\n",
      "AUC: 0.544231997935864\n",
      "AUC: 0.5319319156376721\n",
      "AUC: 0.5468585687014599\n",
      "AUC: 0.523567225001195\n",
      "AUC: 0.5856931741930725\n",
      "AUC: 0.5846240570642044\n",
      "AUC: 0.5566468958906401\n",
      "AUC: 0.566307618677082\n",
      "AUC: 0.5461878943341086\n",
      "Resultados guardados para votes\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "Resultados guardados para avg\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "AUC: 0.5\n",
      "Resultados guardados para median\n"
     ]
    }
   ],
   "source": [
    "from numpy import int64\n",
    "from sklearn.metrics import classification_report, roc_auc_score, multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "METHODS_ENSEMBLE = ['votes', 'avg', 'median']\n",
    "METRICS_NAMES_ENSEMBLE = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1_Score', 'FNR', 'FPR', 'AUC']\n",
    "\n",
    "# Tabla final\n",
    "results_summary = []\n",
    "\n",
    "for method in METHODS_ENSEMBLE:\n",
    "    \n",
    "    # Nombre de carpeta según el método\n",
    "    folder_map = {'votes': 'ensemble_votes', 'avg': 'ensemble_avg', 'median': 'ensemble_median'}\n",
    "    folder = folder_map.get(method, f\"ensemble_{method}\")\n",
    "    \n",
    "    tabla_metodo = []\n",
    "\n",
    "    for dataset in SETS:\n",
    "        fold_metrics = []\n",
    "\n",
    "        for fold in range(1, 6):\n",
    "            # Carga de csv\n",
    "            path_test = os.path.join(DATA_PATH, dataset, f\"test{fold}_{dataset}.csv\")\n",
    "            df_test = pd.read_csv(path_test)\n",
    "            y_true = df_test.iloc[:, -1]    # La columna Result es la clase objetivo\n",
    "\n",
    "            prefix = \"votes\" if method == \"votes\" else method \n",
    "            file_name = f\"{prefix}_fold{fold}_{dataset}.csv\"\n",
    "            path_pred = os.path.join(folder, file_name)\n",
    "            \n",
    "            try:\n",
    "                df_pred = pd.read_csv(path_pred, index_col=\"Index\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Archivo no encontrado: {path_pred}\")\n",
    "                continue\n",
    "\n",
    "            y_pred = df_pred['Ensemble_Decision']\n",
    "            # Probabilidades para AUC\n",
    "            y_proba = df_pred[CLASSES].values\n",
    "\n",
    "            \n",
    "            # Calculamos el reporte general con sklearn\n",
    "            report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "            \n",
    "            accuracy = report['accuracy']\n",
    "            sensitivity = report['macro avg']['recall']\n",
    "            precision = report['macro avg']['precision']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "\n",
    "            # Calculamos el resto de métricas con la matriz de confusión\n",
    "            mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "            tn = mcm[:, 0, 0]\n",
    "            tp = mcm[:, 1, 1]\n",
    "            fn = mcm[:, 1, 0]\n",
    "            fp = mcm[:, 0, 1]\n",
    "\n",
    "            specificity = np.mean(tn / (tn + fp)) if np.any(tn + fp) else 0\n",
    "            fnr = np.mean(fn / (fn + tp)) if np.any(fn + tp) else 0\n",
    "            fpr = np.mean(fp / (fp + tn)) if np.any(fp + tn) else 0\n",
    "\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')\n",
    "            except ValueError:\n",
    "                auc = 0.5 \n",
    "\n",
    "            metrics = [accuracy, sensitivity, specificity, precision, f1, fnr, fpr, auc]\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "        # Promediar folds\n",
    "        if fold_metrics:\n",
    "            df_folds = pd.DataFrame(fold_metrics, columns=METRICS_NAMES_ENSEMBLE)\n",
    "            mean = df_folds.mean().values\n",
    "            std = df_folds.std().values\n",
    "\n",
    "            # Formatear para tabla (Media +- Desv)\n",
    "            fila = [dataset]\n",
    "            for m, s in zip(mean, std):\n",
    "                fila.append(f\"{m:.3f} $\\\\pm$ {s:.3f}\")\n",
    "            tabla_metodo.append(fila)\n",
    "\n",
    "    # Guardar resultados de este método\n",
    "    df_res = pd.DataFrame(tabla_metodo, columns=[\"Set\"] + METRICS_NAMES_ENSEMBLE)\n",
    "    \n",
    "    out_csv = os.path.join(RESULTS_PATH, f\"ensemble_{method}_metrics.csv\")\n",
    "    out_tex = os.path.join(RESULTS_PATH, f\"ensemble_{method}_metrics.tex\")\n",
    "    \n",
    "    df_res.to_csv(out_csv, index=False)\n",
    "    df_res.to_latex(out_tex, index=False, caption=f\"Resultados Ensemble: {method}\")\n",
    "    \n",
    "    print(f\"Resultados guardados para {method}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
