{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a86671",
   "metadata": {},
   "source": [
    "En este script se realizan las predicciones de los modelos entrenados, sobre el conjunto de test.\n",
    "\n",
    "Primero comenzamos importando las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fdec78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d9dcb",
   "metadata": {},
   "source": [
    "Definimos las rutas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4d2f0285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "#CWD = os.path.dirname(__file__)\n",
    "TEST_DATA_PATH = os.path.join(CWD, \"dataset\", \"test\")\n",
    "\n",
    "# Ruta donde se guardaron los entrenamientos del paso anterior\n",
    "RUNS_PATH = os.path.join(CWD, \"runs\")\n",
    "\n",
    "# nombres de las clases ordenados alfabeticamente\n",
    "CLASS_NAMES = sorted(os.listdir(TEST_DATA_PATH))\n",
    "print(CLASS_NAMES)\n",
    "\n",
    "\n",
    "# Ruta donde guardar matrices de confusión\n",
    "TEST_RESULTS_PATH = os.path.join(CWD, \"test_results\")\n",
    "\n",
    "if not os.path.exists(TEST_RESULTS_PATH):\n",
    "    print(f\"Creando directorio {TEST_RESULTS_PATH}.\")\n",
    "    os.mkdir(TEST_RESULTS_PATH)\n",
    "\n",
    "# Nombres de las métricas que vamos a obtener de cada modelo\n",
    "METRICS_NAMES = [\"acc\", \"recall\", \"precision\", \"FNR\", \"FPR\", \"specificity\", \"F1\", \"AUC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd26783",
   "metadata": {},
   "source": [
    "A continuación, se define una función que calcula la predicciones del modelo $y\\_pred$, y las devuelve junto con la clasificación real $y\\_true$, y el $y\\_score$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "59f6f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model_path, test_path, class_list):\n",
    "    \"\"\"Compute predictions with the trained model and return y_true, y_pred, y_score\"\"\"\n",
    "\n",
    "    model = YOLO(model_path)\n",
    "    # print(model.names)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "    \n",
    "    # Iteramos sobre cada carpeta de clase (MEL, NV, etc.)\n",
    "    for class_name in CLASS_NAMES:\n",
    "        class_dir = os.path.join(test_path, class_name)\n",
    "        if not os.path.isdir(class_dir): continue\n",
    "            \n",
    "        # Obtenemos el índice numérico de la clase (ej: MEL = 0)\n",
    "        if class_name in class_list:\n",
    "            class_idx = class_list.index(class_name)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Listamos imágenes\n",
    "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n",
    "\n",
    "        # Predecimos\n",
    "        results = model.predict(images, verbose=False, imgsz=224)\n",
    "\n",
    "        for res in results:\n",
    "            # probs.top1 devuelve el índice de la clase con mayor probabilidad\n",
    "            pred_idx = res.probs.top1\n",
    "            y_score.append(res.probs.data.cpu().numpy())\n",
    "            y_true.append(class_idx)\n",
    "            y_pred.append(pred_idx)\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36f181",
   "metadata": {},
   "source": [
    "Se define una función que genera la matríz de confusión y la guarda para poder ser utilizada más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bebefb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_true, y_pred, model_name):\n",
    "    \"\"\"Function to generate the confusion matrix of the model given its predictions\"\"\"\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, \n",
    "        display_labels=CLASS_NAMES\n",
    "    )\n",
    "    disp.plot()\n",
    "\n",
    "    # Guardamos la matriz de confunsión en una imagen (p.ej \"CM_yolo11n.png\")\n",
    "    plt.savefig(os.path.join(TEST_RESULTS_PATH, f\"CM_{model_name}.png\"))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20f3e4",
   "metadata": {},
   "source": [
    "Se define un método que genera la curva ROC, la guarda y la devuelve para poder ser utilizada más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7661220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ROC_curve_and_AUC(y_true, y_score, model_name):\n",
    "    \"\"\"\n",
    "    Function to generate the ROC curve of the model using the \"One vs Rest\" approach and\n",
    "    return the mean of the AUC values for each positive class\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for class_id in range(len(CLASS_NAMES)):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score[:, class_id], pos_label=class_id)  \n",
    "        ax.plot(\n",
    "            fpr, tpr, \n",
    "            label=f\"ROC curve - Positive class: {CLASS_NAMES[class_id]}\"\n",
    "        )\n",
    "\n",
    "    ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=f\"Curva ROC One-vs-Rest: {model_name}\"\n",
    "    )\n",
    "    \n",
    "    ax.legend()\n",
    "    fig.savefig(os.path.join(TEST_RESULTS_PATH, f\"ROC_{model_name}.png\")) \n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return np.mean(roc_auc_score(y_true, y_score, average=None, multi_class=\"ovr\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a6fc1",
   "metadata": {},
   "source": [
    "Se define una función que calcula y devuelve las métricas de rendimiento del modelo el la fase de predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dfc1156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    \"\"\"Function to compute all the necessary metrics using the \"One vs Rest\" approach\"\"\"\n",
    "    mcm = multilabel_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    tn = mcm[:, 0, 0]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    recall = np.mean(tp / (tp + fn))\n",
    "    precision = np.mean(tp / (tp + fp))\n",
    "    fnr = np.mean(fn / (fn + tp))\n",
    "    fpr = np.mean(fp / (fp + tn))\n",
    "    specificity = 1 - fpr\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return accuracy, recall, precision, fnr, fpr, specificity, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a32cb",
   "metadata": {},
   "source": [
    "Para cada modelo, se va a calcular su rendimiento en base a sus predicciones. El rendimiento de cada modelo será guardado para poder ser comparado más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "813e51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------Evaluando yolo11n-------------------\n",
      "Ruta del modelo: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Aprendizaje_Automatico\\practicas-aprendizaje\\practica4\\runs\\train_yolo11n-cls.pt\\weights\\best.pt\n",
      "\n",
      "-------------------Evaluando yolo11s-------------------\n",
      "Ruta del modelo: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Aprendizaje_Automatico\\practicas-aprendizaje\\practica4\\runs\\train_yolo11s-cls.pt\\weights\\best.pt\n",
      "\n",
      "-------------------Evaluando yolo11m-------------------\n",
      "Ruta del modelo: c:\\Users\\gonza\\Desktop\\universidad\\Malaga\\Tercero\\Aprendizaje_Automatico\\practicas-aprendizaje\\practica4\\runs\\train_yolo11m-cls.pt\\weights\\best.pt\n",
      "\n",
      "Todas las métricas se han calculado y guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "models = [\"yolo11n\", \"yolo11s\", \"yolo11m\"]\n",
    "\n",
    "for variant in models:\n",
    "    # Construimos path al archivo best.pt\n",
    "    model_path = os.path.join(RUNS_PATH, f'train_{variant}-cls.pt', 'weights', 'best.pt')\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"No se encontró el modelo {variant} en {model_path}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n-------------------Evaluando {variant}-------------------\")\n",
    "    print(f\"Ruta del modelo: {model_path}\")\n",
    "\n",
    "    # Obtener las predicciones para el conjunto de test\n",
    "    y_true, y_pred, y_score = make_predictions(model_path, TEST_DATA_PATH, CLASS_NAMES)\n",
    "\n",
    "    # print(y_true)\n",
    "    # print(y_pred)\n",
    "\n",
    "\n",
    "    # Matriz de confusión del modelo\n",
    "    get_confusion_matrix(y_true, y_pred, model_name=variant)\n",
    "    \n",
    "    # Calculamos todas las métricas de rendimiento necesarias\n",
    "    acc, recall, precision, fnr, fpr, specificity, f1 = \\\n",
    "        compute_metrics(y_true=y_true, y_pred=y_pred)\n",
    "    \n",
    "    # Obtenemos la curva ROC\n",
    "    auc = get_ROC_curve_and_AUC(y_true, y_score, model_name=variant)\n",
    "\n",
    "    # Guardamos las métricas\n",
    "    metrics[variant] = [acc, recall, precision, fnr, fpr, specificity, f1, auc]\n",
    "\n",
    "    \n",
    "# Creamos un dataframe con todas las métricas\n",
    "df_metrics = pd.DataFrame(\n",
    "    metrics,\n",
    "    index=METRICS_NAMES\n",
    ").T\n",
    "\n",
    "# Convertimos el dataframe a latex y a csv\n",
    "df_metrics.to_latex(\n",
    "    os.path.join(TEST_RESULTS_PATH, \"metrics.tex\"),\n",
    "    caption=\"Rendimiento de las tres versiones de YOLO\"\n",
    ")\n",
    "\n",
    "df_metrics.to_csv(os.path.join(TEST_RESULTS_PATH, \"metrics.csv\"))\n",
    "\n",
    "print(\"\\nTodas las métricas se han calculado y guardado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
