{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a86671",
   "metadata": {},
   "source": [
    "En este script se realizan las predicciones de los modelos entrenados, sobre el conjunto de test.\n",
    "\n",
    "Primero comenzamos importando las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdec78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e96e1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.47s)\n",
      "creating index...\n",
      "index created!\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Rutas proporcionadas\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "DATASET_DIR = ROOT_DIR / \"dataset\"\n",
    "IMAGES_DIR = DATASET_DIR / \"val2017\"  \n",
    "ANNOTATIONS_FILE = DATASET_DIR / \"annotations\" / \"instances_val2017.json\"\n",
    "RESULTS_DIR = ROOT_DIR / \"validation_results\"\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Cargar anotaciones una sola vez\n",
    "coco = COCO(ANNOTATIONS_FILE)\n",
    "existing_img_ids = os.listdir(IMAGES_DIR)\n",
    "img_ids = [img_id for img_id in coco.getImgIds() if f\"{img_id:012}.jpg\" in existing_img_ids]\n",
    "\n",
    "print(len(img_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8854251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    return intersection / union if union > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278ea83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, img_ids, coco, images_dir, conf_threshold_manual=0.25):\n",
    "    model = YOLO(model_name)\n",
    "    \n",
    "    # --- CREAR MAPEO DE CLASES (CRUCIAL PARA mAP) ---\n",
    "    # Obtenemos los IDs de categorías de COCO ordenados (1, 2, 3... 90)\n",
    "    # YOLOv8 entrenado en COCO asigna el índice 0 al primer ID de COCO (1), el 1 al segundo, etc.\n",
    "    coco_cat_ids = sorted(coco.getCatIds())\n",
    "    # Mapeo: Índice YOLO (0-79) -> ID COCO (1-90)\n",
    "    yolo_to_coco_map = {i: cat_id for i, cat_id in enumerate(coco_cat_ids)}\n",
    "    \n",
    "    # --- Variables para Métricas Manuales ---\n",
    "    all_tp = 0\n",
    "    all_fp = 0\n",
    "    all_fn = 0\n",
    "    \n",
    "    results_list = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for img_id in tqdm(img_ids, desc=f\"Evaluando {model_name}\"):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(images_dir, img_info['file_name'])\n",
    "        \n",
    "        # 1. Ground Truth (Formatos)\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Guardamos bbox y category_id para manual\n",
    "        gt_data = [] \n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            gt_data.append({\n",
    "                'bbox': [x, y, x+w, y+h], # xyxy para cálculo manual\n",
    "                'cat_id': ann['category_id'],\n",
    "                'matched': False\n",
    "            })\n",
    "        \n",
    "        # 2. Inferencia (conf baja para mAP)\n",
    "        results = model(img_path, verbose=False, conf=0.001)[0]\n",
    "        pred_boxes_all = results.boxes\n",
    "        inference_times.append(results.speed['inference'])\n",
    "\n",
    "\n",
    "        # --- PROCESAMIENTO ---\n",
    "        for i, box in enumerate(pred_boxes_all):\n",
    "            # Datos básicos\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            conf = float(box.conf[0].cpu().numpy())\n",
    "            cls_idx = int(box.cls[0].cpu().numpy())\n",
    "            \n",
    "            # --- CORRECCIÓN DE ID ---\n",
    "            # Si el modelo predice una clase fuera del rango COCO (raro pero posible en custom), lo saltamos\n",
    "            if cls_idx not in yolo_to_coco_map:\n",
    "                continue\n",
    "            coco_real_id = yolo_to_coco_map[cls_idx]\n",
    "            \n",
    "            # 1. Guardar para COCOeval (Usando el ID corregido y formato xywh)\n",
    "            results_list.append({\n",
    "                'image_id': img_id,\n",
    "                'category_id': coco_real_id, \n",
    "                'bbox': [x1, y1, w, h],\n",
    "                'score': conf\n",
    "            })\n",
    "            \n",
    "            # 2. Cálculo Manual (Solo si supera umbral de confianza)\n",
    "            if conf >= conf_threshold_manual:\n",
    "                p_box = [x1, y1, x2, y2]\n",
    "                match_found = False\n",
    "                \n",
    "                # Buscamos match en GT\n",
    "                for gt in gt_data:\n",
    "                    # Chequeamos IoU Y TAMBIÉN LA CLASE (Importante)\n",
    "                    if not gt['matched'] and gt['cat_id'] == coco_real_id:\n",
    "                        if calculate_iou(p_box, gt['bbox']) >= 0.5:\n",
    "                            gt['matched'] = True\n",
    "                            match_found = True\n",
    "                            all_tp += 1\n",
    "                            break\n",
    "                \n",
    "                if not match_found:\n",
    "                    all_fp += 1\n",
    "        \n",
    "        # Los que no hicieron match en esta imagen son FN\n",
    "        all_fn += sum(1 for gt in gt_data if not gt['matched'])\n",
    "\n",
    "    # --- MÉTRICAS FINALES ---\n",
    "    precision = all_tp / (all_tp + all_fp) if (all_tp + all_fp) > 0 else 0\n",
    "    recall = all_tp / (all_tp + all_fn) if (all_tp + all_fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    avg_time = np.mean(inference_times)\n",
    "\n",
    "    map50 = 0.0\n",
    "    map50_95 = 0.0\n",
    "    \n",
    "    if len(results_list) > 0:\n",
    "        coco_dt = coco.loadRes(results_list)\n",
    "        coco_eval = COCOeval(coco, coco_dt, 'bbox')\n",
    "        coco_eval.params.imgIds = img_ids\n",
    "        \n",
    "        with redirect_stdout(io.StringIO()):\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "            coco_eval.summarize()\n",
    "            \n",
    "        map50_95 = coco_eval.stats[0]\n",
    "        map50 = coco_eval.stats[1]\n",
    "    \n",
    "    return precision, recall, f1, map50, map50_95, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75cb20f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando yolov8n.pt: 100%|██████████| 100/100 [00:06<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando yolov8s.pt: 100%|██████████| 100/100 [00:10<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando yolov8m.pt: 100%|██████████| 100/100 [00:24<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando yolov8l.pt: 100%|██████████| 100/100 [00:46<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando yolov8x.pt: 100%|██████████| 100/100 [00:58<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>mAP@50</th>\n",
       "      <th>mAP@50-95</th>\n",
       "      <th>Avg Inference Time (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yolov8x.pt</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.617</td>\n",
       "      <td>575.3247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yolov8l.pt</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.617</td>\n",
       "      <td>452.6824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yolov8m.pt</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.596</td>\n",
       "      <td>232.8931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yolov8s.pt</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.537</td>\n",
       "      <td>97.4258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yolov8n.pt</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.456</td>\n",
       "      <td>49.4698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Modelo  Precision  Recall  F1-Score  mAP@50  mAP@50-95  \\\n",
       "4  yolov8x.pt      0.722   0.683     0.702   0.805      0.617   \n",
       "3  yolov8l.pt      0.734   0.660     0.695   0.794      0.617   \n",
       "2  yolov8m.pt      0.688   0.665     0.677   0.777      0.596   \n",
       "1  yolov8s.pt      0.693   0.608     0.648   0.694      0.537   \n",
       "0  yolov8n.pt      0.713   0.553     0.623   0.624      0.456   \n",
       "\n",
       "   Avg Inference Time (ms)  \n",
       "4                 575.3247  \n",
       "3                 452.6824  \n",
       "2                 232.8931  \n",
       "1                  97.4258  \n",
       "0                  49.4698  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_variants = ['n', 's', 'm', 'l', 'x']\n",
    "benchmark_results = []\n",
    "\n",
    "for var in model_variants:\n",
    "    m_name = f\"yolov8{var}.pt\"\n",
    "    # Nota: Puedes ajustar el umbral manual aquí si quieres (por defecto 0.25)\n",
    "    p, r, f1, map50, map50_95, avg_time = evaluate_model(m_name, img_ids, coco, IMAGES_DIR, conf_threshold_manual=0.25)\n",
    "    \n",
    "    benchmark_results.append({\n",
    "        \"Modelo\": m_name,\n",
    "        \"Precision\": round(p, 3),\n",
    "        \"Recall\": round(r, 3),\n",
    "        \"F1-Score\": round(f1, 3),\n",
    "        \"mAP@50\": round(map50, 3),\n",
    "        \"mAP@50-95\": round(map50_95, 3),\n",
    "        \"Avg Inference Time (ms)\": round(avg_time, 4)\n",
    "    })\n",
    "\n",
    "# Crear DataFrame y mostrar ordenado por mAP@50-95\n",
    "df_results = pd.DataFrame(benchmark_results)\n",
    "df_results.sort_values(by=\"mAP@50-95\", ascending=False, inplace=True)\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07ece0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(RESULTS_DIR / \"metrics_comparison.csv\", index=False)\n",
    "df_results.to_latex(RESULTS_DIR / \"metrics_comparison.tex\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
